Какие логи стоит собирать:
DEBUG:
запросы в базу (для мест где нет ПД)
INFO:
Всё что пишется в метрики (для того, чтобы можно было грепать процентили\долгие запросы, аггрегированная визуализация метрик != чтение конкретного значения метрики)
глобальный трейс внутри сервиса и если возможно наружний
все возможные id всех объектов
Действия изменений статусов заказов и пользователей
Все действия операторов
Конфиги сервисов на старте
ERROR:
4хх\5хх запросы
ошибки в обработке бизнес логики
CRITICAL
падения сервиса с необработанной ошибкой (coredump)

Мотивация:
    Возможность точно узнать место падения в коде
    Возможность находить неочевидные связи между действиями пользователя
    Возможность искать места с узкой производительностью
    Единственный реальный способ отслеживать краевые случаи

Предполагаемое решение
cout + logrotate + s3 с ограничением хранения по времени
Если есть много денег yandex cloud logging
Если вдруг система пишет террабайт логов и нельзя настроить сайдкарт с logrotate на каждом сервисе то приоритет
    Seller API
    Shop API
    Mes API
https://disk.yandex.ru/d/udx6L3eCHTto4w

Если мы говорим о небольших данных, то обходимся полнотекстовым поиском чрез grep

Если данных очень много, то раскладываем во что-то имеющее индекс mongo\clickhouse\dwh-like(greenplum) система

Если мы говорим о том, что у нас недостаточно данных в трейсинге для построения бизнес воронок, то строим витрины исходя из потребностей бизнеса.
Думаем о том каких реальных метрик не хватает, чтобы можно было смотреть на воронки

Безопасность данных:
хорошим решением при наличии персональных данных будет хранение данных в обфусцированном виде для разработчиков и в чистом виде для специализированных безопасников.
если возможно сделать без потери полноты информации - делать обфускацию данных ещё на этапе записи, чтобы перс данные вообще не проливались
если получается, что персональные данные всегда требуются для того, чтобы понять что произошло с заказом, значит с архитектурой и использованием данных всё совсем не так